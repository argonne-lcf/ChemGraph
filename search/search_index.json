{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Overview","text":"<p>ChemGraph</p> <p>ChemGraph is an agentic framework that can automate molecular simulation workflows using large language models (LLMs). Built on top of <code>LangGraph</code> and <code>ASE</code>, ChemGraph allows users to perform complex computational chemistry tasks, from structure generation to thermochemistry calculations, with a natural language interface. </p> <p>ChemGraph</p> <p>ChemGraph supports diverse simulation backends, including ab initio quantum chemistry methods (e.g. coupled-cluster, DFT via NWChem, ORCA), semi-empirical methods (e.g., XTB via TBLite), and machine learning potentials (e.g, MACE, UMA) through a modular integration with <code>ASE</code>.</p>"},{"location":"acknowledgements/","title":"Acknowledgements","text":"<p>Info</p> <p>This research used resources of the Argonne Leadership Computing Facility, a U.S. Department of Energy (DOE) Office of Science user facility at Argonne National Laboratory and is based on research supported by the U.S. DOE Office of Science- Advanced Scientific Computing Research Program, under Contract No. DE-AC02- 06CH11357. Our work leverages ALCF Inference Endpoints, which provide a robust API for LLM inference on ALCF HPC clusters via Globus Compute. We are thankful to Serkan Altunta\u015f for his contributions to the user interface of ChemGraph and for insightful discussions on AIOps.</p>"},{"location":"citation/","title":"Citation","text":"<p>If you use ChemGraph in your research, please cite our work:</p> <pre><code>```bibtex\n@article{pham2025chemgraph,\ntitle={ChemGraph: An Agentic Framework for Computational Chemistry Workflows},\nauthor={Pham, Thang D and Tanikanti, Aditya and Ke\u00e7eli, Murat},\njournal={arXiv preprint arXiv:2506.06363},\nyear={2025}\nurl={https://arxiv.org/abs/2506.06363}\n}\n```\n</code></pre>"},{"location":"code_formatting_and_linting/","title":"Code Formatting & Linting","text":"<p>This project uses Ruff for both formatting and linting. To ensure all code follows our style guidelines, install the pre-commit hook:</p> <pre><code>pip install pre-commit\npre-commit install\n</code></pre>"},{"location":"configuration_with_toml/","title":"Configuration with TOML","text":"<p>Note</p> <p>ChemGraph supports comprehensive configuration through TOML files, allowing you to customize model settings, API configurations, chemistry parameters, and more.</p>"},{"location":"configuration_with_toml/#configuration-file-structure","title":"Configuration File Structure","text":"<p>Create a <code>config.toml</code> file in your project directory to configure ChemGraph behavior:</p> <pre><code># ChemGraph Configuration File\n# This file contains all configuration settings for ChemGraph CLI and agents\n\n[general]\n# Default model to use for queries\nmodel = \"gpt-4o-mini\"\n# Workflow type: single_agent, multi_agent, python_repl, graspa\nworkflow = \"single_agent\"\n# Output format: state, last_message\noutput = \"state\"\n# Enable structured output\nstructured = false\n# Generate detailed reports\nreport = true\n\n# Recursion limit for agent workflows\nrecursion_limit = 20\n# Enable verbose output\nverbose = false\n\n[llm]\n# Temperature for LLM responses (0.0 to 1.0)\ntemperature = 0.1\n# Maximum tokens for responses\nmax_tokens = 4000\n# Top-p sampling parameter\ntop_p = 0.95\n# Frequency penalty (-2.0 to 2.0)\nfrequency_penalty = 0.0\n# Presence penalty (-2.0 to 2.0)\npresence_penalty = 0.0\n\n[api]\n# Custom base URLs for different providers\n[api.openai]\nbase_url = \"https://api.openai.com/v1\"\ntimeout = 30\n\n[api.anthropic]\nbase_url = \"https://api.anthropic.com\"\ntimeout = 30\n\n[api.google]\nbase_url = \"https://generativelanguage.googleapis.com/v1beta\"\ntimeout = 30\n\n[api.local]\n# For local models like Ollama\nbase_url = \"http://localhost:11434\"\ntimeout = 60\n\n[chemistry]\n# Default calculation settings\n[chemistry.optimization]\n# Optimization method: BFGS, L-BFGS-B, CG, etc.\nmethod = \"BFGS\"\n# Force tolerance for convergence\nfmax = 0.05\n# Maximum optimization steps\nsteps = 200\n\n[chemistry.frequencies]\n# Displacement for finite difference\ndisplacement = 0.01\n# Number of processes for parallel calculation\nnprocs = 1\n\n[chemistry.calculators]\n# Default calculator for different tasks\ndefault = \"mace_mp\"\n# Available calculators: mace_mp, emt, nwchem, orca, psi4, tblite\nfallback = \"emt\"\n\n[output]\n# Output file settings\n[output.files]\n# Default output directory\ndirectory = \"./chemgraph_output\"\n# File naming pattern\npattern = \"{timestamp}_{query_hash}\"\n# Supported formats: xyz, json, html, png\nformats = [\"xyz\", \"json\", \"html\"]\n\n[output.visualization]\n# 3D visualization settings\nenable_3d = true\n# Molecular viewer: py3dmol, ase_gui\nviewer = \"py3dmol\"\n# Image resolution for saved figures\ndpi = 300\n\n[logging]\n# Logging level: DEBUG, INFO, WARNING, ERROR, CRITICAL\nlevel = \"INFO\"\n# Log file location\nfile = \"./chemgraph.log\"\n# Enable console logging\nconsole = true\n\n[features]\n# Enable experimental features\nenable_experimental = false\n# Enable caching of results\nenable_cache = true\n# Cache directory\ncache_dir = \"./cache\"\n# Cache expiration time in hours\ncache_expiry = 24\n\n[security]\n# Enable API key validation\nvalidate_keys = true\n# Enable request rate limiting\nrate_limit = true\n# Max requests per minute\nmax_requests_per_minute = 60\n\n# Environment-specific configurations\n[environments]\n[environments.development]\nmodel = \"gpt-4o-mini\"\ntemperature = 0.2\nverbose = true\nenable_cache = false\n\n[environments.production]\nmodel = \"gpt-4o\"\ntemperature = 0.1\nverbose = false\nenable_cache = true\nrate_limit = true\n\n[environments.testing]\nmodel = \"gpt-4o-mini\"\ntemperature = 0.0\nverbose = true\nenable_cache = false\nmax_tokens = 1000\n</code></pre>"},{"location":"configuration_with_toml/#using-configuration-files","title":"Using Configuration Files","text":""},{"location":"configuration_with_toml/#with-the-command-line-interface","title":"With the Command Line Interface","text":"<pre><code># Use configuration file\nchemgraph --config config.toml -q \"What is the SMILES string for water?\"\n\n# Override specific settings\nchemgraph --config config.toml -q \"Optimize methane\" -m gpt-4o --verbose\n</code></pre>"},{"location":"configuration_with_toml/#environment-specific-configuration","title":"Environment-Specific Configuration","text":"<p>Set the <code>CHEMGRAPH_ENV</code> environment variable to use environment-specific settings:</p> <pre><code># Use development environment settings\nexport CHEMGRAPH_ENV=development\nchemgraph --config config.toml -q \"Your query\"\n\n# Use production environment settings\nexport CHEMGRAPH_ENV=production\nchemgraph --config config.toml -q \"Your query\"\n</code></pre>"},{"location":"configuration_with_toml/#configuration-sections","title":"Configuration Sections","text":"Section Description <code>[general]</code> Basic settings like model, workflow, and output format <code>[llm]</code> LLM-specific parameters (temperature, max_tokens, etc.) <code>[api]</code> API endpoints and timeouts for different providers <code>[chemistry]</code> Chemistry-specific calculation settings <code>[output]</code> Output file formats and visualization settings <code>[logging]</code> Logging configuration and verbosity levels <code>[features]</code> Feature flags and experimental settings <code>[security]</code> Security settings and rate limiting <code>[environments]</code> Environment-specific configuration overrides"},{"location":"configuration_with_toml/#command-line-interface","title":"Command Line Interface","text":"<p>ChemGraph includes a powerful command-line interface (CLI) that provides all the functionality of the web interface through the terminal. The CLI features rich formatting, interactive mode, and comprehensive configuration options.</p>"},{"location":"configuration_with_toml/#installation-setup","title":"Installation &amp; Setup","text":"<p>The CLI is included by default when you install ChemGraph:</p> <pre><code>pip install -e .\n</code></pre>"},{"location":"configuration_with_toml/#basic-usage","title":"Basic Usage","text":""},{"location":"configuration_with_toml/#quick-start","title":"Quick Start","text":"<pre><code># Basic query\nchemgraph -q \"What is the SMILES string for water?\"\n\n# With model selection\nchemgraph -q \"Optimize methane geometry\" -m gpt-4o\n\n# With report generation\nchemgraph -q \"Calculate CO2 vibrational frequencies\" -r\n\n# Using configuration file\nchemgraph --config config.toml -q \"Your query here\"\n</code></pre>"},{"location":"configuration_with_toml/#command-syntax","title":"Command Syntax","text":"<pre><code>chemgraph [OPTIONS] -q \"YOUR_QUERY\"\n</code></pre>"},{"location":"configuration_with_toml/#command-line-options","title":"Command Line Options","text":"<p>Core Arguments:</p> Option Short Description Default <code>--query</code> <code>-q</code> The computational chemistry query to execute Required <code>--model</code> <code>-m</code> LLM model to use <code>gpt-4o-mini</code> <code>--workflow</code> <code>-w</code> Workflow type <code>single_agent</code> <code>--output</code> <code>-o</code> Output format (<code>state</code>, <code>last_message</code>) <code>state</code> <code>--structured</code> <code>-s</code> Use structured output format <code>False</code> <code>--report</code> <code>-r</code> Generate detailed report <code>False</code> <p>Model Selection:</p> <pre><code># OpenAI models\nchemgraph -q \"Your query\" -m gpt-4o\nchemgraph -q \"Your query\" -m gpt-4o-mini\nchemgraph -q \"Your query\" -m o1-preview\n\n# Anthropic models\nchemgraph -q \"Your query\" -m claude-3-5-sonnet-20241022\nchemgraph -q \"Your query\" -m claude-3-opus-20240229\n\n# Google models\nchemgraph -q \"Your query\" -m gemini-1.5-pro\n\n# Local models (requires vLLM server)\nchemgraph -q \"Your query\" -m llama-3.1-70b-instruct\n</code></pre> <p>Workflow Types:</p> <pre><code># Single agent (default) - best for most tasks\nchemgraph -q \"Optimize water molecule\" -w single_agent\n\n# Multi-agent - complex tasks with planning\nchemgraph -q \"Complex analysis\" -w multi_agent\n\n# Python REPL - interactive coding\nchemgraph -q \"Write analysis code\" -w python_repl\n\n# gRASPA - molecular simulation\nchemgraph -q \"Run adsorption simulation\" -w graspa\n</code></pre> <p>Output Formats:</p> <pre><code># Full state output (default)\nchemgraph -q \"Your query\" -o state\n\n# Last message only\nchemgraph -q \"Your query\" -o last_message\n\n# Structured output\nchemgraph -q \"Your query\" -s\n\n# Generate detailed report\nchemgraph -q \"Your query\" -r\n</code></pre>"},{"location":"configuration_with_toml/#interactive-mode","title":"Interactive Mode","text":"<p>Start an interactive session for continuous conversations:</p> <pre><code>chemgraph --interactive\n</code></pre> <p>Interactive Features: - Persistent conversation: Maintain context across queries - Model switching: Change models mid-conversation - Workflow switching: Switch between different agent types - Built-in commands: Help, clear, config, etc.</p> <p>Interactive Commands: <pre><code># In interactive mode, type:\nhelp                    # Show available commands\nclear                   # Clear screen\nconfig                  # Show current configuration\nquit                    # Exit interactive mode\nmodel gpt-4o           # Change model\nworkflow multi_agent   # Change workflow\n</code></pre></p>"},{"location":"configuration_with_toml/#utility-commands","title":"Utility Commands","text":"<p>List Available Models: <pre><code>chemgraph --list-models\n</code></pre></p> <p>Check API Keys: <pre><code>chemgraph --check-keys\n</code></pre></p> <p>Get Help: <pre><code>chemgraph --help\n</code></pre></p>"},{"location":"configuration_with_toml/#configuration-file-support","title":"Configuration File Support","text":"<p>Use TOML configuration files for consistent settings:</p> <pre><code>chemgraph --config config.toml -q \"Your query\"\n</code></pre>"},{"location":"configuration_with_toml/#environment-variables","title":"Environment Variables","text":"<p>Set environment-specific configurations:</p> <pre><code># Use development settings\nexport CHEMGRAPH_ENV=development\nchemgraph --config config.toml -q \"Your query\"\n\n# Use production settings\nexport CHEMGRAPH_ENV=production\nchemgraph --config config.toml -q \"Your query\"\n</code></pre>"},{"location":"configuration_with_toml/#advanced-options","title":"Advanced Options","text":"<p>Timeout and Error Handling: <pre><code># Set recursion limit\nchemgraph -q \"Complex query\" --recursion-limit 30\n\n# Verbose output for debugging\nchemgraph -q \"Your query\" -v\n\n# Save output to file\nchemgraph -q \"Your query\" --output-file results.txt\n</code></pre></p>"},{"location":"configuration_with_toml/#example-workflows","title":"Example Workflows","text":"<p>Basic Molecular Analysis: <pre><code># Get molecular structure\nchemgraph -q \"What is the SMILES string for caffeine?\"\n\n# Optimize geometry\nchemgraph -q \"Optimize the geometry of caffeine using DFT\" -m gpt-4o -r\n\n# Calculate properties\nchemgraph -q \"Calculate the vibrational frequencies of optimized caffeine\" -r\n</code></pre></p> <p>Interactive Research Session: <pre><code># Start interactive mode\nchemgraph --interactive\n\n# Select model and workflow\n&gt; model gpt-4o\n&gt; workflow single_agent\n\n# Conduct analysis\n&gt; What is the structure of aspirin?\n&gt; Optimize its geometry using DFT\n&gt; Calculate its electronic properties\n&gt; Compare with ibuprofen\n</code></pre></p> <p>Batch Processing: <pre><code># Process multiple queries\nchemgraph -q \"Analyze water molecule\" --output-file water_analysis.txt\nchemgraph -q \"Analyze methane molecule\" --output-file methane_analysis.txt\nchemgraph -q \"Analyze ammonia molecule\" --output-file ammonia_analysis.txt\n</code></pre></p>"},{"location":"configuration_with_toml/#api-key-setup","title":"API Key Setup","text":"<p>Required API Keys: <pre><code># OpenAI (for GPT models)\nexport OPENAI_API_KEY=\"your_openai_key_here\"\n\n# Anthropic (for Claude models)\nexport ANTHROPIC_API_KEY=\"your_anthropic_key_here\"\n\n# Google (for Gemini models)\nexport GEMINI_API_KEY=\"your_gemini_key_here\"\n</code></pre></p> <p>Getting API Keys: - OpenAI: Visit platform.openai.com/api-keys - Anthropic: Visit console.anthropic.com - Google: Visit aistudio.google.com/apikey</p>"},{"location":"configuration_with_toml/#performance-tips","title":"Performance Tips","text":"<ul> <li>Use <code>gpt-4o-mini</code> for faster, cost-effective queries</li> <li>Use <code>gpt-4o</code> for complex analysis requiring higher reasoning</li> <li>Enable <code>--report</code> for detailed documentation</li> <li>Use <code>--structured</code> output for programmatic parsing</li> <li>Leverage configuration files for consistent settings</li> </ul>"},{"location":"configuration_with_toml/#troubleshooting","title":"Troubleshooting","text":"<p>Common Issues: <pre><code># Check API key status\nchemgraph --check-keys\n\n# Verify model availability\nchemgraph --list-models\n\n# Test with verbose output\nchemgraph -q \"test query\" -v\n\n# Check configuration\nchemgraph --config config.toml -q \"test\" --verbose\n</code></pre></p> <p>Error Messages: - \"Invalid model\": Use <code>--list-models</code> to see available options - \"API key not found\": Use <code>--check-keys</code> to verify setup - \"Query required\": Use <code>-q</code> to specify your query - \"Timeout\": Increase <code>--recursion-limit</code> or simplify query</p> <p>The CLI provides: - Beautiful terminal output with colors and formatting powered by Rich - API key validation before agent initialization - Timeout protection to prevent hanging processes - Interactive mode for continuous conversations - Configuration file support with TOML format - Environment-specific settings for development/production - Comprehensive help and examples for all features</p>"},{"location":"docker_support/","title":"Docker Support with Docker Compose (Recommended for vLLM)","text":"<p>Note</p> <p>This project uses Docker Compose to manage multi-container applications, providing a consistent development and deployment environment. This setup allows you to run the <code>chemgraph</code> (with JupyterLab) and a local vLLM model server as separate, inter-communicating services.</p>"},{"location":"docker_support/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker installed on your system.</li> <li>Docker Compose installed on your system.</li> <li>vllm cloned into the project root. <code>git clone https://github.com/vllm-project/vllm.git</code></li> </ul>"},{"location":"docker_support/#overview","title":"Overview","text":"<p>The <code>docker-compose.yml</code> file defines two main services: 1.  <code>jupyter_lab</code>:      *   Builds from the main <code>Dockerfile</code>.     *   Runs JupyterLab, allowing you to interact with the notebooks and agent code.     *   Is configured to communicate with the <code>vllm_server</code>. 2.  <code>vllm_server</code>:     *   Builds from <code>Dockerfile.arm</code> by default (located in the project root), which is suitable for running vLLM on macOS (Apple Silicon / ARM-based CPUs). This Dockerfile is a modified version intended for CPU execution.     *   For other operating systems or hardware (e.g., Linux with NVIDIA GPUs), you will need to use a different Dockerfile. The vLLM project provides a collection of Dockerfiles for various architectures (CPU, CUDA, ROCm, etc.) available at https://github.com/vllm-project/vllm/tree/main/docker. You would need to adjust the <code>docker-compose.yml</code> to point to the appropriate Dockerfile and context (e.g., by cloning the vLLM repository locally and referencing a Dockerfile within it).     *   Starts an OpenAI-compatible API server using vLLM, serving a pre-configured model (e.g., <code>meta-llama/Llama-3-8B-Instruct</code> as per the current <code>docker-compose.yml</code>).     *   Listens on port 8000 within the Docker network (and is exposed to host port 8001 by default).</p> <p>Building and Running with Docker Compose</p> <p>Navigate to the root directory of the project (where <code>docker-compose.yml</code> is located) and run:</p> <pre><code>docker-compose up --build\n</code></pre> <p>Note on Hugging Face Token (<code>HF_TOKEN</code>): Many models, including the default <code>meta-llama/Llama-3-8B-Instruct</code>, are gated and require Hugging Face authentication. To provide your Hugging Face token to the <code>vllm_server</code> service:</p> <ol> <li>Create a <code>.env</code> file in the root directory of the project (the same directory as <code>docker-compose.yml</code>).</li> <li>Add your Hugging Face token to this file:     <pre><code>HF_TOKEN=\"your_actual_hugging_face_token_here\"\n</code></pre></li> </ol> <p>Docker Compose will automatically load this variable when you run <code>docker-compose up</code>. The <code>vllm_server</code> in <code>docker-compose.yml</code> is configured to use this environment variable.</p> <p>Breakdown of the command: - <code>docker-compose up</code>: Starts or restarts all services defined in <code>docker-compose.yml</code>. - <code>--build</code>: Forces Docker Compose to build the images before starting the containers. This is useful if you've made changes to <code>Dockerfile</code>, <code>Dockerfile.arm</code> (or other vLLM Dockerfiles), or project dependencies.</p> <p>After running this command: - The vLLM server will start, and its logs will be streamed to your terminal. - JupyterLab will start, and its logs will also be streamed. JupyterLab will be accessible in your web browser at <code>http://localhost:8888</code>. No token is required by default.</p> <p>To stop the services, press <code>Ctrl+C</code> in the terminal where <code>docker-compose up</code> is running. To stop and remove the containers, you can use <code>docker-compose down</code>.</p>"},{"location":"docker_support/#configuring-notebooks-to-use-the-local-vllm-server","title":"Configuring Notebooks to Use the Local vLLM Server","text":"<p>When you initialize <code>ChemGraph</code> in your Jupyter notebooks (running within the <code>jupyter_lab</code> service), you can now point to the local vLLM server:</p> <ol> <li>Model Name: Use the Hugging Face identifier of the model being served by vLLM (e.g., <code>meta-llama/Llama-3-8B-Instruct</code> as per default in <code>docker-compose.yml</code>).</li> <li>Base URL &amp; API Key: These are automatically passed as environment variables (<code>VLLM_BASE_URL</code> and <code>OPENAI_API_KEY</code>) to the <code>jupyter_lab</code> service by <code>docker-compose.yml</code>. The agent code in <code>llm_agent.py</code> has been updated to automatically use these environment variables if a model name is provided that isn't in the pre-defined supported lists (OpenAI, Ollama, ALCF, Anthropic).</li> </ol> <p>Example in a notebook:</p> <pre><code>from chemgraph.agent.llm_agent import ChemGraph\n\n# The model name should match what vLLM is serving.\n# The base_url and api_key will be picked up from environment variables\n# set in docker-compose.yml if this model_name is not a standard one.\nagent = ChemGraph(\n    model_name=\"meta-llama/Llama-3-8B-Instruct\", # Or whatever model is configured in docker-compose.yml\n    workflow_type=\"single_agent\", \n    # No need to explicitly pass base_url or api_key here if using the docker-compose setup\n)\n\n# Now you can run the agent\n# response = agent.run(\"What is the SMILES string for water?\")\n# print(response)\n</code></pre> <p>The <code>jupyter_lab</code> service will connect to <code>http://vllm_server:8000/v1</code> (as defined by <code>VLLM_BASE_URL</code> in <code>docker-compose.yml</code>) to make requests to the language model.</p>"},{"location":"docker_support/#gpu-support-for-vllm-advanced","title":"GPU Support for vLLM (Advanced)","text":"<p>The provided <code>Dockerfile.arm</code> and the default <code>docker-compose.yml</code> setup are configured for CPU-based vLLM (suitable for macOS). To enable GPU support (typically on Linux with NVIDIA GPUs):</p> <ol> <li>Choose the Correct vLLM Dockerfile:<ul> <li>Do not use <code>Dockerfile.arm</code>.</li> <li>You will need to use a Dockerfile from the official vLLM repository designed for CUDA. Clone the vLLM repository (e.g., into a <code>./vllm</code> subdirectory in your project) or use it as a submodule.</li> <li>A common choice is <code>vllm/docker/Dockerfile</code> (for CUDA) or a specific version like <code>vllm/docker/Dockerfile.cuda-12.1</code>. Refer to vLLM Dockerfiles for options.</li> </ul> </li> <li> <p>Modify <code>docker-compose.yml</code>:</p> <ul> <li>Change the <code>build.context</code> for the <code>vllm_server</code> service to point to your local clone of the vLLM repository (e.g., <code>./vllm</code>).</li> <li>Change the <code>build.dockerfile</code> to the path of the CUDA-enabled Dockerfile within that context (e.g., <code>docker/Dockerfile</code>).</li> <li>Uncomment and configure the <code>deploy.resources.reservations.devices</code> section for the <code>vllm_server</code> service to grant it GPU access.</li> </ul> <p><pre><code># ... in docker-compose.yml, for vllm_server:\n# build:\n#   context: ./vllm  # Path to your local vLLM repo clone\n#   dockerfile: docker/Dockerfile # Path to the CUDA Dockerfile within the vLLM repo\n# ...\n# environment:\n  # Remove or comment out:\n  # - VLLM_CPU_ONLY=1 \n  # ...\ndeploy:\n  resources:\n    reservations:\n      devices:\n        - driver: nvidia\n          count: 1 # or 'all'\n          capabilities: [gpu]\n</code></pre> 3.  NVIDIA Container Toolkit: Ensure you have the NVIDIA Container Toolkit installed on your host system for Docker to recognize and use NVIDIA GPUs. 4.  Build Arguments: Some official vLLM Dockerfiles accept build arguments (e.g., <code>CUDA_VERSION</code>, <code>PYTHON_VERSION</code>). You might need to pass these via the <code>build.args</code> section in <code>docker-compose.yml</code>.</p> <p><pre><code># ... in docker-compose.yml, for vllm_server build:\n# args:\n#   - CUDA_VERSION=12.1.0 \n#   - PYTHON_VERSION=3.10 \n</code></pre> Consult the specific vLLM Dockerfile you choose for available build arguments.</p> </li> </ol>"},{"location":"docker_support/#running-only-jupyterlab-for-external-llm-services","title":"Running Only JupyterLab (for External LLM Services)","text":"<p>If you prefer to use external LLM services like OpenAI, Claude, or other hosted providers instead of running a local vLLM server, you can run only the JupyterLab service:</p> <pre><code>docker-compose up jupyter_lab\n</code></pre> <p>This will start only the JupyterLab container without the vLLM server. In this setup:</p> <ol> <li>JupyterLab Access: JupyterLab will be available at <code>http://localhost:8888</code></li> <li>LLM Configuration: In your notebooks, configure the agent to use external services by providing appropriate model names and API keys:</li> </ol> <p>Example for OpenAI: <pre><code>import os\nfrom chemgraph.agent.llm_agent import ChemGraph\n\n# Set your OpenAI API key as an environment variable or pass it directly\nos.environ[\"OPENAI_API_KEY\"] = \"your-openai-api-key-here\"\n\nagent = ChemGraph(\n    model_name=\"gpt-4\",  # or \"gpt-3.5-turbo\", \"gpt-4o\", etc.\n    workflow_type=\"single_agent\"\n)\n</code></pre></p> <p>Example for Anthropic Claude: <pre><code>import os\nfrom chemgraph.agent.llm_agent import ChemGraph\n\n# Set your Anthropic API key\nos.environ[\"ANTHROPIC_API_KEY\"] = \"your-anthropic-api-key-here\"\n\nagent = ChemGraph(\n    model_name=\"claude-3-sonnet-20240229\",  # or other Claude models\n    workflow_type=\"single_agent_ase\"\n)\n</code></pre></p> <p>Available Environment Variables for External Services: - <code>OPENAI_API_KEY</code>: For OpenAI models - <code>ANTHROPIC_API_KEY</code>: For Anthropic Claude models - <code>GEMINI_API_KEY</code>: For Gemini models</p>"},{"location":"docker_support/#working-with-example-notebooks","title":"Working with Example Notebooks","text":"<p>Once JupyterLab is running (via <code>docker-compose up</code> or <code>docker-compose up jupyter_lab</code>), you can navigate to the <code>notebooks/</code> directory within the JupyterLab interface to open and run the example notebooks. Modify them as shown above to use either the locally served vLLM model or external LLM services.</p>"},{"location":"docker_support/#notes-on-tblite-python-api","title":"Notes on TBLite Python API","text":"<p>The <code>tblite</code> package is installed via pip within the <code>jupyter_lab</code> service. For the full Python API functionality of TBLite (especially for XTB), you might need to follow separate installation instructions as mentioned in the TBLite documentation. If you require this, you may need to modify the main <code>Dockerfile</code> to include these additional installation steps or perform them inside a running container and commit the changes to a new image for the <code>jupyter_lab</code> service.</p>"},{"location":"example_usage/","title":"Example Usage","text":"<p>Note</p> <p>Before exploring example usage in the <code>notebooks/</code> directory, ensure you have specified the necessary API tokens in your environment. </p> OpenAI API KeyAnthropic API KeyGoogle AI Studio (Gemini) API Key <ol> <li> <p>Log in to your OpenAI account at the OpenAI Platform website. If you don't have an account, you'll need to create one first.</p> </li> <li> <p>Navigate to the API keys section. You can find this by clicking on your profile icon in the top-right corner and selecting \"API keys.\"</p> </li> <li> <p>Click the + Create new secret key button.</p> </li> <li> <p>Give your key a descriptive name (e.g., \"ChemGraph\").</p> </li> <li> <p>Click Create secret key. A new key will be generated.</p> </li> <li> <p>Copy the key and save it in a secure location. You will not be able to see it again after this step.</p> </li> <li> <p>Set the key in your environment using the command provided in the instructions:      <pre><code>export OPENAI_API_KEY=\"your_api_key_here\"  # On Unix or macOS\nsetx OPENAI_API_KEY \"your_api_key_here\"  # On Windows\n</code></pre></p> </li> <li>Restart your terminal or IDE to ensure the environment variable is loaded.</li> </ol> <ol> <li> <p>Sign up or log in to your Anthropic account at the Anthropic console.</p> </li> <li> <p>In the left-hand navigation menu, select API Keys.</p> </li> <li> <p>Click on the option to create a new API key.</p> </li> <li> <p>Provide a name for your API key (e.g., \"ChemGraph\").</p> </li> <li> <p>Click Create Key again.</p> </li> <li> <p>Copy the generated key and store it securely, as you may not be able to view it again.</p> </li> <li> <p>Set the key in your environment using the command provided in the instructions:      <pre><code>export ANTHROPIC_API_KEY=\"your_api_key_here\"  # On Unix or macOS\nsetx ANTHROPIC_API_KEY \"your_api_key_here\"  # On Windows\n</code></pre></p> </li> <li>Restart your terminal or IDE to ensure the environment variable is loaded.</li> </ol> <ol> <li> <p>Go to the Google AI Studio website at Google AI Studio and sign in with your Google account.</p> </li> <li> <p>In the left-hand menu, select Get API key.</p> </li> <li> <p>Click the Create API key in new project button. A new key will be instantly generated.</p> </li> <li> <p>Copy the API key by clicking the copy icon next to it.</p> </li> <li> <p>Set the key as an environment variable:      <pre><code>export GOOGLE_API_KEY=\"your_api_key_here\"  # On Unix or macOS\nsetx GOOGLE_API_KEY \"your_api_key_here\"  # On Windows\n</code></pre></p> </li> <li>Restart your terminal or IDE to ensure the environment variable is loaded.</li> </ol> Explore Example Notebooks <p>Navigate to the <code>notebooks/</code> directory to explore various example notebooks demonstrating different capabilities of ChemGraph.</p> <ul> <li> <p>Single-Agent System with MACE: This notebook demonstrates how a single agent can utilize multiple tools with MACE/xTB support.</p> </li> <li> <p>Single-Agent System with UMA: This notebook demonstrates how a single agent can utilize multiple tools with UMA support.</p> </li> <li> <p>Multi-Agent System: This notebook demonstrates a multi-agent setup where different agents (Planner, Executor and Aggregator) handle various tasks exemplifying the collaborative potential of ChemGraph.</p> </li> <li> <p>Single-Agent System with gRASPA: This notebook provides a sample guide on executing a gRASPA simulation using a single agent. For gRASPA-related installation instructions, visit the gRASPA GitHub repository. The notebook's functionality has been validated on a single compute node at ALCF Polaris.</p> </li> </ul>"},{"location":"installation/","title":"Installation","text":"<p>Note</p> <p>Ensure you have Python 3.10 or higher installed on your system.</p> <p>Note on Compatibility with conda</p> <ul> <li>ChemGraph supports both MACE and UMA (Meta's machine learning potential). However, due to the current dependency conflicts, particularly with <code>e3nn</code>\u2014you cannot install both in the same environment.  </li> <li>To use both libraries, create separate Conda environments, one for each.</li> </ul> install with pipinstall with condainstall with uv <ul> <li>Clone the repository:      <pre><code>git clone https://github.com/Autonomous-Scientific-Agents/ChemGraph\ncd ChemGraph\n</code></pre></li> <li> <p>Create and activate a virtual environment:      <pre><code># Using venv (built into Python)\npython -m venv chemgraph-env\nsource chemgraph-env/bin/activate  # On Unix/macOS\n# OR\n.\\chemgraph-env\\Scripts\\activate  # On Windows\n</code></pre></p> </li> <li> <p>Install ChemGraph:      <pre><code>pip install -e .\n</code></pre></p> </li> </ul> <p>Option 1: Using environment.yml (Recommended)</p> <ul> <li> <p>Clone the repository:      <pre><code>git clone https://github.com/Autonomous-Scientific-Agents/ChemGraph\ncd ChemGraph\n</code></pre></p> </li> <li> <p>Create and activate the conda environment from the provided environment.yml:      <pre><code>conda env create -f environment.yml\nconda activate chemgraph\n</code></pre></p> </li> <li> <p>Install additional conda dependencies if needed:      <pre><code>conda install -c conda-forge nwchem\n</code></pre></p> </li> </ul> <p>Option 2: Manual conda setup</p> <ul> <li> <p>Clone the repository:      <pre><code>git clone https://github.com/Autonomous-Scientific-Agents/ChemGraph\ncd ChemGraph\n</code></pre></p> </li> <li> <p>Create and activate a new Conda environment:      <pre><code>conda create -n chemgraph python=3.10 -y\nconda activate chemgraph\n</code></pre></p> </li> <li> <p>Install required Conda dependencies:       <pre><code>conda install -c conda-forge nwchem\n</code></pre></p> </li> <li> <p>Install <code>ChemGraph</code> and its dependencies:</p> </li> </ul> <ul> <li> <p>Clone the repository:      <pre><code>git clone https://github.com/Autonomous-Scientific-Agents/ChemGraph\ncd ChemGraph\n</code></pre></p> </li> <li> <p>Create and activate a virtual environment using uv:      <pre><code>uv venv chemgraph-env\nuv venv --python 3.11 chemgraph-env # For specific python version\n\nsource chemgraph-env/bin/activate # Unix/macos\n.\\chemgraph-env\\Scripts\\activate  # On Windows\n</code></pre></p> </li> <li> <p>Install ChemGraph using uv:      <pre><code>uv pip install -e .\n</code></pre></p> </li> </ul> <p>Optional: Install with UMA support</p> <ul> <li>Note on e3nn Conflict for UMA Installation: The <code>uma</code> extras (requiring <code>e3nn&gt;=0.5</code>) conflict with the base <code>mace-torch</code> dependency (which pins <code>e3nn==0.4.4</code>). </li> <li> <p>If you need to install UMA support in an environment where <code>mace-torch</code> might cause this conflict, you can try the following workaround:</p> <ol> <li>Temporarily modify <code>pyproject.toml</code>: Open the <code>pyproject.toml</code> file in the root of the ChemGraph project.</li> <li>Find the line containing <code>\"mace-torch&gt;=0.3.13\",</code> in the <code>dependencies</code> list.</li> <li>Comment out this line by adding a <code>#</code> at the beginning (e.g., <code>#    \"mace-torch&gt;=0.3.13\",</code>).</li> <li>Install UMA extras: Run <code>pip install -e \".[uma]\"</code>.</li> <li>(Optional) Restore <code>pyproject.toml</code>: After installation, you can uncomment the <code>mace-torch</code> line if you still need it for other purposes in the same environment. Be aware that <code>mace-torch</code> might not function correctly due to the <code>e3nn</code> version mismatch (<code>e3nn&gt;=0.5</code> will be present for UMA).</li> </ol> </li> <li> <p>The most robust solution for using both MACE and UMA with their correct dependencies is to create separate Conda environments, as highlighted in the \"Note on Compatibility\" above.</p> </li> <li> <p>Important for UMA Model Access: The <code>facebook/UMA</code> model is a gated model on Hugging Face. To use it, you must:</p> <ol> <li>Visit the facebook/UMA model page on Hugging Face.</li> <li>Log in with your Hugging Face account.</li> <li>Accept the model's terms and conditions if prompted.</li> </ol> </li> <li>Your environment (local or CI) must also be authenticated with Hugging Face, typically by logging in via <code>huggingface-cli login</code> or ensuring <code>HF_TOKEN</code> is set and recognized.</li> </ul> <pre><code>pip install -e \".[uma]\"\n</code></pre>"},{"location":"license/","title":"License","text":"<p>Info</p> <p>This project is licensed under the Apache 2.0 License.</p>"},{"location":"project_structure/","title":"Project Structure","text":"<pre><code>chemgraph/\n\u2502\n\u251c\u2500\u2500 src/                       # Source code\n\u2502   \u251c\u2500\u2500 chemgraph/             # Top-level package\n\u2502   \u2502   \u251c\u2500\u2500 agent/             # Agent-based task management\n\u2502   \u2502   \u251c\u2500\u2500 graphs/            # Workflow graph utilities\n\u2502   \u2502   \u251c\u2500\u2500 models/            # Different Pydantic models\n\u2502   \u2502   \u251c\u2500\u2500 prompt/            # Agent prompt\n\u2502   \u2502   \u251c\u2500\u2500 state/             # Agent state\n\u2502   \u2502   \u251c\u2500\u2500 tools/             # Tools for molecular simulations\n\u2502   \u2502   \u251c\u2500\u2500 utils/             # Other utility functions\n\u2502\n\u251c\u2500\u2500 pyproject.toml             # Project configuration\n\u2514\u2500\u2500 README.md                  # Project documentation\n</code></pre>"},{"location":"running_local_models/","title":"Running Local Models with vLLM","text":"<p>Note</p> <p>This section describes how to set up and run local language models using the vLLM inference server.</p>"},{"location":"running_local_models/#inference-backend-setup-remotelocal","title":"Inference Backend Setup (Remote/Local)","text":""},{"location":"running_local_models/#virtual-python-environment","title":"Virtual Python Environment","text":"<p>All instructions below must be executed within a Python virtual environment. Ensure the virtual environment uses the same Python version as your project (e.g., Python 3.11).</p> <p>Example 1: Using conda <pre><code>conda create -n vllm-env python=3.11 -y\nconda activate vllm-env\n</code></pre></p> <p>Example 2: Using python venv <pre><code>python3.11 -m venv vllm-env\nsource vllm-env/bin/activate  # On Windows use `vllm-env\\\\Scripts\\\\activate`\n</code></pre></p>"},{"location":"running_local_models/#install-inference-server-vllm","title":"Install Inference Server (vLLM)","text":"<p>vLLM is recommended for serving many transformer models efficiently.</p> <p>Basic vLLM installation from source: Make sure your virtual environment is activated. <pre><code># Ensure git is installed\ngit clone https://github.com/vllm-project/vllm.git\ncd vllm\npip install -e .\n</code></pre> For specific hardware acceleration (e.g., CUDA, ROCm), refer to the official vLLM installation documentation.</p>"},{"location":"running_local_models/#running-the-vllm-server-standalone","title":"Running the vLLM Server (Standalone)","text":"<p>A script is provided at <code>scripts/run_vllm_server.sh</code> to help start a vLLM server with features like logging, retry attempts, and timeout. This is useful for running vLLM outside of Docker Compose, for example, directly on a machine with GPU access.</p> <p>Before running the script: 1.  Ensure your vLLM Python virtual environment is activated.     <pre><code># Example: if you used conda\n# conda activate vllm-env \n# Example: if you used python venv\n# source path/to/your/vllm-env/bin/activate\n</code></pre> 2.  Make the script executable:     <pre><code>chmod +x scripts/run_vllm_server.sh\n</code></pre></p> <p>To run the script:</p> <pre><code>./scripts/run_vllm_server.sh [MODEL_IDENTIFIER] [PORT] [MAX_MODEL_LENGTH]\n</code></pre> <ul> <li><code>[MODEL_IDENTIFIER]</code> (optional): The Hugging Face model identifier. Defaults to <code>facebook/opt-125m</code>.</li> <li><code>[PORT]</code> (optional): The port for the vLLM server. Defaults to <code>8001</code>.</li> <li><code>[MAX_MODEL_LENGTH]</code> (optional): The maximum model length. Defaults to <code>4096</code>.</li> </ul> <p>Example: <pre><code>./scripts/run_vllm_server.sh meta-llama/Meta-Llama-3-8B-Instruct 8001 8192\n</code></pre></p> Important Note on Gated Models (e.g., Llama 3): <ul> <li> <p>Many models, such as those from the Llama family by Meta, are gated and require you to accept their terms of use on Hugging Face and use an access token for download. </p> </li> <li> <p>To use such models with vLLM (either via the script or Docker Compose):</p> <ol> <li>Hugging Face Account and Token: Ensure you have a Hugging Face account and have generated an access token with <code>read</code> permissions. You can find this in your Hugging Face account settings under \"Access Tokens\".</li> <li>Accept Model License: Navigate to the Hugging Face page of the specific model you want to use (e.g., <code>meta-llama/Meta-Llama-3-8B-Instruct</code>) and accept its license/terms if prompted.</li> <li>Environment Variables: Before running the vLLM server (either via the script or <code>docker-compose up</code>), you need to set the following environment variables in your terminal session or within your environment configuration (e.g., <code>.bashrc</code>, <code>.zshrc</code>, or by passing them to Docker Compose if applicable):     <pre><code>export HF_TOKEN=\"your_hugging_face_token_here\"\n# Optional: Specify a directory for Hugging Face to download models and cache.\n# export HF_HOME=\"/path/to/your/huggingface_cache_directory\"\n</code></pre>     vLLM will use these environment variables to authenticate with Hugging Face and download the model weights.</li> </ol> </li> <li> <p>The script will:</p> <ul> <li>Attempt to start the vLLM OpenAI-compatible API server.</li> <li>Log output to a file in the <code>logs/</code> directory (created if it doesn't exist at the project root).</li> <li>The server runs in the background via <code>nohup</code>.</li> </ul> </li> <li> <p>This standalone script is an alternative to running vLLM via Docker Compose and is primarily for users who manage their vLLM instances directly.</p> </li> </ul>"},{"location":"streamlit_web_interface/","title":"Streamlit Web Interface","text":"<p>Note</p> <p>ChemGraph includes a Streamlit web interface that provides an intuitive, chat-based UI for interacting with computational chemistry agents. The interface supports 3D molecular visualization, conversation history, and easy access to various ChemGraph workflows.</p>"},{"location":"streamlit_web_interface/#features","title":"Features","text":"<ul> <li>\ud83e\uddea Interactive Chat Interface: Natural language queries for computational chemistry tasks</li> <li>\ud83e\uddec 3D Molecular Visualization: Interactive molecular structure display using <code>stmol</code> and <code>py3Dmol</code></li> <li>\ud83d\udcca Report Integration: Embedded HTML reports from computational calculations</li> <li>\ud83d\udcbe Data Export: Download molecular structures as XYZ or JSON files</li> <li>\ud83d\udd27 Multiple Workflows: Support for single-agent, multi-agent, Python REPL, and gRASPA workflows</li> <li>\ud83c\udfa8 Modern UI: Clean, responsive interface with conversation bubbles and molecular properties display</li> </ul>"},{"location":"streamlit_web_interface/#installation-requirements","title":"Installation Requirements","text":"<p>The Streamlit UI dependencies are included by default when you install ChemGraph:</p> <pre><code># Install ChemGraph (includes UI dependencies)\npip install -e .\n</code></pre> <p>Alternative Installation Options: <pre><code># Install only UI dependencies separately (if needed)\npip install -e \".[ui]\"\n\n# Install with UMA support (separate environment recommended)\npip install -e \".[uma]\"\n</code></pre></p>"},{"location":"streamlit_web_interface/#running-the-streamlit-interface","title":"Running the Streamlit Interface","text":"<ol> <li> <p>Set up your API keys (same as for notebooks):    <pre><code>export OPENAI_API_KEY=\"your_openai_api_key_here\"\nexport ANTHROPIC_API_KEY=\"your_anthropic_api_key_here\"\n</code></pre></p> </li> <li> <p>Launch the Streamlit app:    <pre><code>streamlit run ui/app.py\n</code></pre></p> </li> <li> <p>Access the interface: Open your browser to <code>http://localhost:8501</code></p> </li> </ol>"},{"location":"streamlit_web_interface/#using-the-interface","title":"Using the Interface","text":""},{"location":"streamlit_web_interface/#configuration","title":"Configuration","text":"<ul> <li>Model Selection: Choose from GPT-4o, GPT-4o-mini, or Claude models</li> <li>Workflow Type: Select single-agent, multi-agent, Python REPL, or gRASPA workflows</li> </ul>"},{"location":"streamlit_web_interface/#interaction","title":"Interaction","text":"<ol> <li>Initialize Agent: Click \"Initialize Agent\" in the sidebar to set up your ChemGraph instance</li> <li>Ask Questions: Use the text area to enter computational chemistry queries</li> <li>View Results: See responses in chat bubbles with automatic structure detection</li> <li>3D Visualization: When molecular structures are detected, they're automatically displayed in 3D</li> <li>Download Data: Export structures and calculation results directly from the interface</li> </ol>"},{"location":"streamlit_web_interface/#example-queries","title":"Example Queries","text":"<ul> <li>\"What is the SMILES string for caffeine?\"</li> <li>\"Optimize the geometry of water molecule using DFT\"</li> <li>\"Calculate the single point energy of methane and show the structure\"</li> <li>\"Generate the structure of aspirin and calculate its vibrational frequencies\"</li> </ul>"},{"location":"streamlit_web_interface/#molecular-visualization","title":"Molecular Visualization","text":"<p>The interface automatically detects molecular structure data in agent responses and provides: - Interactive 3D Models: Multiple visualization styles (ball &amp; stick, sphere, stick, wireframe) - Structure Information: Chemical formula, composition, mass, center of mass - Export Options: Download as XYZ files or JSON data - Fallback Display: Table view when 3D visualization is unavailable</p>"},{"location":"streamlit_web_interface/#conversation-management","title":"Conversation Management","text":"<ul> <li>History Display: All queries and responses are preserved in conversation bubbles</li> <li>Structure Detection: Molecular structures are automatically extracted and visualized</li> <li>Report Integration: HTML reports from calculations are embedded directly in the interface</li> <li>Debug Information: Expandable sections show detailed message processing information</li> </ul>"},{"location":"streamlit_web_interface/#troubleshooting","title":"Troubleshooting","text":"<p>3D Visualization Issues: - Ensure <code>stmol</code> is installed: <code>pip install stmol</code> - If 3D display fails, the interface falls back to table/text display - Check browser compatibility for WebGL support</p> <p>Agent Initialization: - Verify API keys are set correctly - Check that ChemGraph package is installed: <code>pip install -e .</code> - Ensure all dependencies are available in your environment</p> <p>Performance: - For large molecular systems, visualization may take longer to load - Use the refresh button if the interface becomes unresponsive - Clear conversation history to improve performance with many queries</p>"}]}