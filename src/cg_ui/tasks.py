import asyncio
from typing import AsyncIterator, Dict, Any
from chemgraph.agent.llm_agent import ChemGraph
import os
from langchain_core.messages import AIMessage, ToolMessage
import json
from ase.data import chemical_symbols
import uuid


def atoms_data_to_xyz(atoms_data: Dict[str, Any]) -> str:
    """Converts AtomsData dictionary to an XYZ format string."""
    try:
        numbers = atoms_data["numbers"]
        positions = atoms_data["positions"]

        xyz_lines = [str(len(numbers)), "Generated by ChemGraph UI"]

        for num, pos in zip(numbers, positions):
            symbol = chemical_symbols[num]
            xyz_lines.append(f"{symbol} {pos[0]:.6f} {pos[1]:.6f} {pos[2]:.6f}")

        return "\n".join(xyz_lines)
    except (KeyError, IndexError) as e:
        return f"Error converting AtomsData to XYZ: {e}"


async def run_workflow_async(prompt: str) -> AsyncIterator[Dict[str, Any]]:
    """Runs a ChemGraph workflow asynchronously and streams results."""
    yield {"status": "queued"}

    try:
        cg = ChemGraph(
            model_name="gpt-4o-mini",
            workflow_type="single_agent",
            structured_output=True,
            return_option="state",
            generate_report=True,
        )
    except Exception as e:
        yield {"status": "error", "result": f"Failed to initialize ChemGraph: {e}"}
        return

    yield {"status": "running"}

    input_payload = {"messages": [("user", prompt)]}
    config = {"configurable": {"thread_id": str(uuid.uuid4())}}

    full_log = []
    final_state = None
    final_content = None

    async for chunk in cg.workflow.astream(input_payload, config=config):
        node_name = list(chunk.keys())[0]
        node_output = chunk[node_name]

        log_entry = f"Node '{node_name}'"
        yield {"log": log_entry}
        full_log.append(log_entry)

        if "messages" in node_output:
            message = node_output["messages"][-1]
            if isinstance(message, AIMessage):
                if message.content:
                    yield {"token": message.content}
                    full_log.append(f"AI: {message.content}")
                if message.tool_calls:
                    log_entry = (
                        f"  -> Calls tools: {[tc['name'] for tc in message.tool_calls]}"
                    )
                    yield {"log": log_entry}
                    full_log.append(log_entry)
            elif isinstance(message, ToolMessage):
                log_entry = f"Tool '{message.name}' output: {message.content[:200]}..."
                yield {"log": log_entry}
                full_log.append(log_entry)
            else:  # Formatted output can be a stringified dict
                final_content = str(message)

        final_state = chunk

    # After stream, process final result
    if final_content:
        result_data = None
        try:
            result_data = json.loads(final_content)
        except (json.JSONDecodeError, TypeError):
            result_data = {"answer": str(final_content)}

        if result_data and "answer" in result_data:
            answer = result_data["answer"]
            if (
                isinstance(answer, dict)
                and "numbers" in answer
                and "positions" in answer
            ):
                xyz_str = atoms_data_to_xyz(answer)
                result_data["mol_xyz"] = xyz_str

        yield {"status": "done", "result": result_data, "log": "\n".join(full_log)}
    else:
        yield {
            "status": "error",
            "result": "Workflow finished with no output.",
            "log": "\n".join(full_log),
        }
